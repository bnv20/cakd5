{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"using-word-embeddings.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1ksMCaluMRGeI9RTQ0jB4RKutWndYP8d8","authorship_tag":"ABX9TyNH0AvSEcPvNoygqQcPXbv5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["### GloVe 단어 임베딩 내려받기\n","\n","https://nlp.stanford.edu/projects/glove 에서 2014년 영문 위키피디아를 사용해 사전에 계산된 임베딩을 내려받습니다. 이 파일의 이름은 glove.6B.zip이고 압축 파일 크기는 823MB입니다. 400,000만개의 단어(또는 단어가 아닌 토큰)에 대한 100차원의 임베딩 벡터를 포함하고 있습니다. datasets 폴더 아래에 파일 압축을 해제합니다.\n"],"metadata":{"id":"TJcB35DlrFpM"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zZVM0szehq_j","executionInfo":{"status":"ok","timestamp":1649816076017,"user_tz":-540,"elapsed":8507,"user":{"displayName":"kevin park","userId":"02703084888761299921"}},"outputId":"1e416028-c23c-412b-a940-1cc88daec6ed"},"outputs":[{"output_type":"stream","name":"stdout","text":["400000개의 단어 벡터\n"]}],"source":["import os\n","import numpy as np\n","glove_dir = 'drive/MyDrive/cakd5_colab/m9_딥러닝알고리즘구현/datasets/'\n","\n","embeddings_index = {}\n","f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'), encoding='utf8')\n","for line in f:\n","  values = line.split()\n","  word = values[0]\n","  coefs = np.asarray(values[1:], dtype='float32')\n","  embeddings_index[word] = coefs\n","f.close()\n","\n","print(f'{len(embeddings_index)}개의 단어 벡터')\n"]},{"cell_type":"markdown","source":["200개의 샘플을 학습한 후 영화 리뷰를 분류"],"metadata":{"id":"fNOeUUNDudbz"}},{"cell_type":"code","source":[""],"metadata":{"id":"ZvmfndPwuqI7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n"],"metadata":{"id":"9kCXg3-muDkV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embedding_dim = 100\n","\n","embedding_matrix = np.zeros((max_words, embedding_dim))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":175},"id":"10s9bnsPs1kC","executionInfo":{"status":"error","timestamp":1649816252654,"user_tz":-540,"elapsed":411,"user":{"displayName":"kevin park","userId":"02703084888761299921"}},"outputId":"676f0829-3d9c-4228-afef-905155711cb1"},"execution_count":5,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-5f0d8aa4f20b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'word_index' is not defined"]}]}]}